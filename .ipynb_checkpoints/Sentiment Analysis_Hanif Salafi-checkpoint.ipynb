{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import six\n",
    "import re  \n",
    "import os\n",
    "import sys\n",
    "import nltk  \n",
    "import pandas\n",
    "import pickle  \n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import warnings\n",
    "import numbers\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from Lib.Contractions import CONTRACTION_MAP\n",
    "from Lib.ProgressBar import ProgressBar\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.datasets import load_files\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from itertools import zip_longest\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "from math import log\n",
    "from distutils.version import LooseVersion\n",
    "from inspect import signature\n",
    "from numpy.core.numeric import ComplexWarning\n",
    "from scipy.sparse import issparse\n",
    "from scipy.special import digamma\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "               L O A D   D A T A S E T              \n",
      "----------------------------------------------------\n",
      "--- LOAD DATASET DONE ✓ ----\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "================================================================================================\n",
    "                                P R E - P R O C E S S I N G\n",
    "================================================================================================\n",
    "\n",
    "''' \n",
    "\n",
    "class Load_Data():\n",
    "    \n",
    "    def make_corpus(self, root_dir):\n",
    "        polarity_dirs = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]    \n",
    "        corpus = []    \n",
    "        for polarity_dir in polarity_dirs:\n",
    "            reviews = [os.path.join(polarity_dir,f) for f in os.listdir(polarity_dir)]\n",
    "            for review in reviews:\n",
    "                doc_string = \"\";\n",
    "                with open(review) as rev:\n",
    "                    for line in rev:\n",
    "                        doc_string = doc_string + line\n",
    "                if not corpus:\n",
    "                    corpus = [doc_string]\n",
    "                else:\n",
    "                    corpus.append(doc_string)\n",
    "\n",
    "        labels = np.zeros(2000)\n",
    "        labels[0:1000] = 0\n",
    "        labels[1000:2000] = 1\n",
    "        \n",
    "        print(\"--- LOAD DATASET DONE \\u2713 ----\")\n",
    "\n",
    "        return corpus, labels\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"----------------------------------------------------\")\n",
    "        print(\"               L O A D   D A T A S E T              \")\n",
    "        print(\"----------------------------------------------------\")\n",
    "        \n",
    "root_dir = 'Data/txt_sentoken'\n",
    "data_train_x, data_train_y = Load_Data().make_corpus(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing - All Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n##  Test Function Preprocessing  ##\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "================================================================================================\n",
    "                                P R E - P R O C E S S I N G\n",
    "================================================================================================\n",
    "\n",
    "''' \n",
    "\n",
    "class Preprocessing():\n",
    "    \n",
    "    def remove_prefix_b(self, data):\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', data)\n",
    "        return document\n",
    "    \n",
    "    def expand_contractions(self, data, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "        contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                          flags=re.IGNORECASE|re.DOTALL)\n",
    "        def expand_match(contraction):\n",
    "            match = contraction.group(0)\n",
    "            first_char = match[0]\n",
    "            expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                    if contraction_mapping.get(match)\\\n",
    "                                    else contraction_mapping.get(match.lower())                       \n",
    "            expanded_contraction = first_char+expanded_contraction[1:]\n",
    "            return expanded_contraction\n",
    "\n",
    "        expanded_text = contractions_pattern.sub(expand_match, data)\n",
    "        expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "        return expanded_text\n",
    "\n",
    "    def remove_special_characters(self, data, remove_digits=False):\n",
    "        pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "        document = re.sub(pattern, '', data)\n",
    "        return document\n",
    "    \n",
    "    def spelling_correction(self, data, status):\n",
    "        if status == False :\n",
    "            return data\n",
    "        blob = TextBlob(data)\n",
    "        document = blob.correct()\n",
    "        return document\n",
    "    \n",
    "    def remove_single_characters(self, data):\n",
    "        # remove all single characters\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', data)\n",
    "        # Remove single characters from the start\n",
    "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "        return document\n",
    "    \n",
    "    def remove_multiplespace(self, data):\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', data, flags=re.I)\n",
    "        return document\n",
    "    \n",
    "    def split_document(self, data):\n",
    "        document = data.split()\n",
    "        return document\n",
    "    \n",
    "    def convert_to_lowercase(self, data):\n",
    "        # Converting to Lowercase\n",
    "        document = data.lower()\n",
    "        return document\n",
    "    \n",
    "    def stop_words_removal(self, data, status):\n",
    "        if status == False :\n",
    "            return data\n",
    "        \n",
    "        stop_words = ['the', 'and', 'of', 'is', 'to', 'in', 'it', 'that', 'as', 'not', 'with', \n",
    "                       'for', 'his', 'this', 'film', 'he', 'but', 'are', 'on', 'by', 'be', 'have', \n",
    "                       'an', 'who', 'one', 'movie', 'you', 'was', 'from', 'at']\n",
    "\n",
    "        document = [word for word in data if not word in stop_words]\n",
    "        return document\n",
    "\n",
    "    def remove_punctuation(self, data):\n",
    "        document = re.sub(r'[^\\w\\s]','', data)\n",
    "        return document  \n",
    "    \n",
    "    def stemming(self, data, status):\n",
    "        if status == False :\n",
    "            return data\n",
    "        # stemmer = LancasterStemmer()\n",
    "        # stemmer = PorterStemmer()\n",
    "        stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "        document = [stemmer.stem(word) for word in data]\n",
    "        return document\n",
    "    \n",
    "    def lemmatization(self, data, status):\n",
    "        if status == False :\n",
    "            return data\n",
    "        # Lemmatization\n",
    "        lemma = WordNetLemmatizer()\n",
    "        document = [lemma.lemmatize(word) for word in data]\n",
    "        return document\n",
    "    \n",
    "    def join_word(self, data):\n",
    "        document = [word for word in data]\n",
    "        document = ' '.join(document)\n",
    "        return document\n",
    "    \n",
    "    def get_result(self):\n",
    "        print(\"--- PREPROCESSING DONE \\u2713 ---\")\n",
    "        return self.result\n",
    "    \n",
    "    def __init__(self, stopword, stem, x, y=None):\n",
    "        print(\"----------------------------------------------------\")\n",
    "        print(\"             P R E - P R O C E S S I N G            \")\n",
    "        print(\"----------------------------------------------------\")\n",
    "        data_train_x, data_train_y = x, y\n",
    "        documents_cleaned = []\n",
    "        \n",
    "        progress = ProgressBar(len(data_train_x), fmt=ProgressBar.FULL)\n",
    "        for index in range(progress.total):\n",
    "            progress.current += 1\n",
    "            progress()\n",
    "            \n",
    "            prep_prefix_b = self.remove_prefix_b(data_train_x[index])\n",
    "            prep_contractions = self.expand_contractions(prep_prefix_b)\n",
    "            prep_special_char = self.remove_special_characters(prep_contractions, remove_digits=True)\n",
    "            prep_single_char = self.remove_single_characters(prep_special_char)\n",
    "            prep_multiplespace = self.remove_multiplespace(prep_single_char)\n",
    "            prep_lowercase = self.convert_to_lowercase(prep_multiplespace)\n",
    "            prep_spell_correction = self.spelling_correction(prep_lowercase, status=False)\n",
    "            prep_punctuation = self.remove_punctuation(prep_spell_correction)\n",
    "            prep_split_data = self.split_document(prep_punctuation)\n",
    "            prep_stopword = self.stop_words_removal(prep_split_data, stopword)\n",
    "            prep_stemming = self.stemming(prep_stopword, stem)\n",
    "            prep_join_word = self.join_word(prep_stemming)\n",
    "            documents_cleaned.append(prep_join_word)\n",
    "        progress.done()  \n",
    "        \n",
    "        if len(y) == 0:\n",
    "            self.result = documents_cleaned\n",
    "        else:\n",
    "            self.result = {}\n",
    "            self.result['review'] = documents_cleaned\n",
    "            self.result['class'] = data_train_y\n",
    "  \n",
    "        \n",
    "'''\n",
    "##  Test Function Preprocessing  ##\n",
    "'''\n",
    "# documents_cleaned = Preprocessing(stopword = True, stem = True, x=data_train_x, y=data_train_y).get_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing - Stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "             P R E - P R O C E S S I N G            \n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================] 2000/2000 (100%)    0 to go\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Result Test Stopword --\n",
      "stopword = 0  --> Akurasi SVM : 0.884\n",
      "stopword = 5  --> Akurasi SVM : 0.883\n",
      "stopword = 10  --> Akurasi SVM : 0.8835\n",
      "stopword = 15  --> Akurasi SVM : 0.884\n",
      "stopword = 20  --> Akurasi SVM : 0.8835\n",
      "stopword = 25  --> Akurasi SVM : 0.8835\n",
      "stopword = 30  --> Akurasi SVM : 0.8845\n",
      "stopword = 35  --> Akurasi SVM : 0.8815\n",
      "stopword = 40  --> Akurasi SVM : 0.8815\n",
      "stopword = 45  --> Akurasi SVM : 0.8815\n",
      "stopword = 50  --> Akurasi SVM : 0.882\n",
      "stopword = 55  --> Akurasi SVM : 0.882\n",
      "stopword = 60  --> Akurasi SVM : 0.883\n",
      "stopword = 65  --> Akurasi SVM : 0.88\n",
      "stopword = 70  --> Akurasi SVM : 0.88\n",
      "stopword = 75  --> Akurasi SVM : 0.878\n",
      "stopword = 80  --> Akurasi SVM : 0.879\n",
      "stopword = 85  --> Akurasi SVM : 0.878\n",
      "stopword = 90  --> Akurasi SVM : 0.8735\n",
      "stopword = 95  --> Akurasi SVM : 0.8775\n",
      "stopword = 100  --> Akurasi SVM : 0.876\n",
      "\n",
      "--- Best Stopword ---\n",
      "Jumlah Stopword: 30 , Akurasi : 0.8845\n",
      "List Word :\n",
      "['the', 'and', 'of', 'is', 'to', 'in', 'it', 'that', 'as', 'not', 'with', 'for', 'his', 'this', 'film', 'he', 'but', 'are', 'on', 'by', 'be', 'have', 'an', 'who', 'one', 'movie', 'you', 'was', 'from', 'at']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "class Stopword():\n",
    "    \n",
    "    def get_top_n_words(self, corpus, n=None):\n",
    "        vec = CountVectorizer().fit(corpus)\n",
    "        bag_of_words = vec.transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        return words_freq[:n]\n",
    "\n",
    "\n",
    "    def stop_words_removal(self, data, stop_words):\n",
    "        new_document = []\n",
    "        for index in range(len(data)):\n",
    "            document = data[index].split()\n",
    "            document = [word for word in document if not word in stop_words]\n",
    "            document = [word for word in document]\n",
    "            document = ' '.join(document)\n",
    "            new_document.append(document)\n",
    "        return new_document\n",
    "\n",
    "    def SVM(self, new_document, documents_cleaned):\n",
    "        kf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "        totalsvm = 0\n",
    "        totalMatSvm = np.zeros((2,2))\n",
    "\n",
    "        corpus = new_document\n",
    "        labels = documents_cleaned['class']\n",
    "\n",
    "        for train_index, test_index in kf.split(corpus, labels):\n",
    "            X_train = [corpus[i] for i in train_index]\n",
    "            X_test = [corpus[i] for i in test_index]\n",
    "            y_train, y_test = labels[train_index], labels[test_index]\n",
    "            vectorizer = TfidfVectorizer(sublinear_tf=True, use_idf=True)\n",
    "            train_corpus_tf_idf = vectorizer.fit_transform(X_train) \n",
    "            test_corpus_tf_idf = vectorizer.transform(X_test)\n",
    "\n",
    "            model1 = LinearSVC()\n",
    "            model1.fit(train_corpus_tf_idf,y_train)\n",
    "            result1 = model1.predict(test_corpus_tf_idf)\n",
    "\n",
    "            totalMatSvm = totalMatSvm + confusion_matrix(y_test, result1)\n",
    "            totalsvm = totalsvm+sum(y_test==result1)\n",
    "\n",
    "\n",
    "        tn, fp, fn, tp = totalMatSvm.ravel()\n",
    "        acc = (tp + tn)/2000\n",
    "        f1 = 2*tp / (2*tp + fp + fn)\n",
    "        print (\" --> Akurasi SVM : {0}\".format(acc))\n",
    "        return acc\n",
    "    \n",
    "    def check_max_accuracy(self, list_stopword, list_num, stopword_acc):\n",
    "        max_acc = max(stopword_acc)\n",
    "        for idx in range(len(list_num)):\n",
    "            if max_acc == stopword_acc[idx]:\n",
    "                print(\"\\n--- Best Stopword ---\")\n",
    "                print(\"Jumlah Stopword:\", list_num[idx], end=\" \")\n",
    "                print(\", Akurasi :\", max_acc)\n",
    "                print(\"List Word :\")\n",
    "                print(list_stopword[idx])\n",
    "                break\n",
    "\n",
    "'''\n",
    "##  Test Stopword  ##\n",
    "'''\n",
    "\n",
    "documents_cleaned = Preprocessing(stopword = False, stem = False, x=data_train_x, y=data_train_y).get_result()\n",
    "list_num = [0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]\n",
    "\n",
    "list_stopword = []\n",
    "stopword_acc = []\n",
    "\n",
    "print(\"--- Result Training Stopword --\")\n",
    "for num in list_num:\n",
    "    print('stopword =',num, end=\" \")\n",
    "    common_words = Stopword().get_top_n_words(documents_cleaned['review'], num)\n",
    "    \n",
    "    stopword = []\n",
    "    for word, freq in common_words:\n",
    "        stopword.append(word)\n",
    "    list_stopword.append(stopword)\n",
    "    \n",
    "    new_document = Stopword().stop_words_removal(documents_cleaned['review'], stopword)\n",
    "    \n",
    "    stopword_acc.append(Stopword().SVM(new_document, documents_cleaned))\n",
    "    \n",
    "print(\"Training Stopword Done \\u2713 \")\n",
    "Stopword().check_max_accuracy(list_stopword, list_num, stopword_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing with Stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "             P R E - P R O C E S S I N G            \n",
      "----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[======================================= ] 1989/2000 ( 99%)   11 to go"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PREPROCESSING DONE ✓ ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[======================================= ] 1990/2000 ( 99%)   10 to go\r",
      "[======================================= ] 1991/2000 ( 99%)    9 to go\r",
      "[======================================= ] 1992/2000 ( 99%)    8 to go\r",
      "[======================================= ] 1993/2000 ( 99%)    7 to go\r",
      "[======================================= ] 1994/2000 ( 99%)    6 to go\r",
      "[======================================= ] 1995/2000 ( 99%)    5 to go\r",
      "[======================================= ] 1996/2000 ( 99%)    4 to go\r",
      "[======================================= ] 1997/2000 ( 99%)    3 to go\r",
      "[======================================= ] 1998/2000 ( 99%)    2 to go\r",
      "[======================================= ] 1999/2000 ( 99%)    1 to go\r",
      "[========================================] 2000/2000 (100%)    0 to go\r",
      "[========================================] 2000/2000 (100%)    0 to go\n"
     ]
    }
   ],
   "source": [
    "documents_cleaned = Preprocessing(stopword = True, stem = False, x=data_train_x, y=data_train_y).get_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "              S P L I T    D A T A S E T            \n",
      "----------------------------------------------------\n",
      "--- SPLIT DATASET DONE ✓ ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Split_Data at 0x25bc078d7f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "================================================================================================\n",
    "                                    S P L I T     D A T A\n",
    "================================================================================================\n",
    "\n",
    "'''\n",
    "\n",
    "class Split_Data():\n",
    "        \n",
    "    def sort_score_feature(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        idx = np.argsort(y)\n",
    "        return X[idx], y[idx]\n",
    "    \n",
    "    def do_split_data(self, data_cleaned, n):\n",
    "        kf = StratifiedKFold(n_splits=n, shuffle=False, random_state=None)\n",
    "\n",
    "        corpus = data_cleaned['review']\n",
    "        labels = data_cleaned['class']\n",
    "        \n",
    "        k=1\n",
    "        for train_index, test_index in kf.split(corpus, labels):\n",
    "            X_train = [corpus[i] for i in train_index]\n",
    "            X_test = [corpus[i] for i in test_index]\n",
    "            y_train, y_test = labels[train_index], labels[test_index]\n",
    "            self.output_split_data(X_train, y_train, k, 'train/datatrain_seleksi_fitur')\n",
    "            self.output_split_data(X_test, y_test, k, 'test/datatest_seleksi_fitur')\n",
    "            k += 1\n",
    "        print(\"--- SPLIT DATASET DONE \\u2713 ---\")\n",
    "\n",
    "    def output_split_data(self, review, class_score, k, filename):\n",
    "        filepath = os.getcwd()+'/Data/Preprocessing/' +filename+str(k)+'.csv'\n",
    "        raw_data = {'Review': review,\n",
    "                   'Class': class_score}\n",
    "        df = pandas.DataFrame(raw_data, columns = ['Review', 'Class'])\n",
    "        df.to_csv(filepath, index=False)\n",
    "            \n",
    "    def __init__(self, data_cleaned, n):\n",
    "        print(\"----------------------------------------------------\")\n",
    "        print(\"              S P L I T    D A T A S E T            \")\n",
    "        print(\"----------------------------------------------------\")  \n",
    "\n",
    "        self.do_split_data(data_cleaned, n)\n",
    "\n",
    "        \n",
    "'''\n",
    "----------------------------------------------------\n",
    "            Test Function Split Data\n",
    "----------------------------------------------------\n",
    "'''\n",
    "Split_Data(documents_cleaned, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "        F E A T U R E       S E L E C T I O N       \n",
      "----------------------------------------------------\n",
      "K- 1  Jumlah Data :  1800\n",
      "# mutual_information #\n",
      "K- 2  Jumlah Data :  1800\n",
      "# mutual_information #\n",
      "K- 3  Jumlah Data :  1800\n",
      "# mutual_information #\n",
      "K- 4  Jumlah Data :  1800\n",
      "# mutual_information #\n",
      "K- 5  Jumlah Data :  1800\n",
      "# mutual_information #\n",
      "K- 6  Jumlah Data :  1800\n",
      "# mutual_information #\n",
      "K- 7  Jumlah Data :  1800\n",
      "# mutual_information #\n",
      "K- 8  Jumlah Data :  1800\n",
      "# mutual_information #\n",
      "K- 9  Jumlah Data :  1800\n",
      "# mutual_information #\n",
      "K- 10  Jumlah Data :  1800\n",
      "# mutual_information #\n",
      "--- FEATURE SELECTION DONE ✓ ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Feature_Selection at 0x25bc078d978>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "================================================================================================\n",
    "                            F E A T U R E       S E L E C T I O N\n",
    "================================================================================================\n",
    "\n",
    "'''\n",
    "\n",
    "class Feature_Selection():\n",
    "    \n",
    "    def input_vocabulary(self, data):\n",
    "        print('# Input Vocabulary #')\n",
    "        vocabulary = []\n",
    "        \n",
    "        progress = ProgressBar(len(data['review']), fmt=ProgressBar.FULL)\n",
    "        for index in range(progress.total):\n",
    "            progress.current += 1\n",
    "            progress()\n",
    "            for word in data['review'][index]:\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary.append(word)\n",
    "        progress.done()\n",
    "        return vocabulary\n",
    "\n",
    "    def num_class(self, feature_data, data):\n",
    "        print('# num_class #')\n",
    "        feature_data['num_class_1'] = 0\n",
    "        feature_data['num_class_0'] = 0\n",
    "        for class_val in data['class']:\n",
    "            if(class_val == 0):\n",
    "                feature_data['num_class_0'] += 1\n",
    "            else:\n",
    "                feature_data['num_class_1'] += 1\n",
    "        return feature_data\n",
    "    \n",
    "    def sort_score_feature(self, feature_data):\n",
    "        list1, list2 = zip(*sorted(zip(feature_data['score_feature'], feature_data['vocabulary']), reverse=True))\n",
    "      \n",
    "        feature_data['score_feature'] = list1\n",
    "        feature_data['vocabulary'] = list2\n",
    "\n",
    "        return feature_data\n",
    "    \n",
    "    def information_gain(self, feature_data, data, k):\n",
    "        print(\"# information_gain #\")\n",
    "        prob_0_total = feature_data['num_class_0'] / len(data['review'])\n",
    "        prob_1_total = feature_data['num_class_1'] / len(data['review'])\n",
    "        log_0_total = math.log(prob_0_total, 2)\n",
    "        log_1_total = math.log(prob_1_total, 2)\n",
    "        entropy_total = -(prob_1_total * log_1_total) -(prob_0_total * log_0_total)\n",
    "        print(entropy_total)\n",
    "        \n",
    "        feature_data['score_feature'] = []\n",
    "        \n",
    "        progress = ProgressBar(len(feature_data['vocabulary']), fmt=ProgressBar.FULL)\n",
    "        for index in range(progress.total):\n",
    "            progress.current += 1\n",
    "            progress()\n",
    "            \n",
    "            vocabulary = feature_data['vocabulary'][index]\n",
    "            \n",
    "            #Entropy Value            \n",
    "            s_value = set([text.count(vocabulary) for text in data['review']])            \n",
    "            sigma_v = 0\n",
    "\n",
    "            for value in s_value:\n",
    "                nol, satu, both = 0, 0, 0\n",
    "                for i in range(0, len(data['review'])):\n",
    "                    if((data['class'][i] == 0) and (data['review'][i].count(vocabulary) == value)):\n",
    "                        nol += 1\n",
    "                    if((data['class'][i] == 1) and (data['review'][i].count(vocabulary) == value)):\n",
    "                        satu += 1\n",
    "                    if(data['review'][i].count(vocabulary) == value):\n",
    "                        both += 1\n",
    "                prob_0_value = 0\n",
    "                if(nol > 0 and both > 0):\n",
    "                    prob_0_value = nol / both\n",
    "                prob_1_value = 0\n",
    "                if(satu > 0 and both > 0):\n",
    "                    prob_1_value = satu / both\n",
    "\n",
    "                log_0_value = 0\n",
    "                log_1_value = 0\n",
    "                if(prob_0_value > 0):\n",
    "                    log_0_value = math.log(prob_0_value, 2)\n",
    "                if(prob_1_value > 0):\n",
    "                    log_1_value = math.log(prob_1_value, 2)\n",
    "                    \n",
    "                entropy_value = (both/len(data['review'])) * (-(prob_1_value * log_1_value) -(prob_0_value * log_0_value))\n",
    "                sigma_v += entropy_value\n",
    "\n",
    "            gain_level = entropy_total - sigma_v\n",
    "            feature_data['score_feature'].append(gain_level)\n",
    "\n",
    "        progress.done()\n",
    "        feature_data = self.sort_score_feature(feature_data)\n",
    "        return feature_data\n",
    "        \n",
    "    def mutual_information(self, feature_data, data):\n",
    "        print(\"# mutual_information #\")\n",
    "        \n",
    "        data_review = []\n",
    "        \n",
    "        for i in range(0,len(data['review'])):\n",
    "            documents = data['review'][i]\n",
    "            document = [word for word in documents]\n",
    "            document = ' '.join(document)\n",
    "            data_review.append(document)\n",
    "        \n",
    "        cv = CountVectorizer()\n",
    "        X_vec = cv.fit_transform(data_review)\n",
    "        Y = data['class']\n",
    "        \n",
    "        # Get MI Score\n",
    "    \n",
    "        X = X_vec.asformat('csc')\n",
    "        y = np.ravel(Y)\n",
    "        \n",
    "        columns = range(X.shape[1])\n",
    "        iterate_column = []\n",
    "        for i in columns:\n",
    "            x = np.zeros(X.shape[0])\n",
    "            start_ptr, end_ptr = X.indptr[i], X.indptr[i + 1]\n",
    "            x[X.indices[start_ptr:end_ptr]] = X.data[start_ptr:end_ptr]\n",
    "            iterate_column.append(x)\n",
    "\n",
    "        mi = []\n",
    "        \n",
    "        for x in iterate_column:            \n",
    "            labels_true, labels_pred = np.asarray(x), np.asarray(y)\n",
    "            #  classes = jumlah kemunculan fitur {1,3,6} , class_idx = urutan kemunculan classes {0, 1, 2}\n",
    "            classes, class_idx = np.unique(labels_true, return_inverse=True)            \n",
    "            #  clusters = kelas fitur {0, 1} , cluster_idx = urutan kemunculan kelas {1, 1,...0, 0}\n",
    "            clusters, cluster_idx = np.unique(labels_pred, return_inverse=True)\n",
    "\n",
    "            # Tabel untuk menghitung total kemunculan setiap nilai fitur di Kelas 0 dan 1\n",
    "            contingency = sp.coo_matrix((np.ones(class_idx.shape[0]),\n",
    "                                        (class_idx, cluster_idx)),\n",
    "                                        shape=(classes.shape[0], clusters.shape[0]),\n",
    "                                        dtype=np.int)\n",
    "\n",
    "            contingency = contingency.tocsr()\n",
    "            contingency.sum_duplicates()\n",
    "\n",
    "            if sp.issparse(contingency):\n",
    "                # Tabel contingency dipisah mejadi 3: nilai fitur, kelas, dan total nilai fitur terhadap kelas\n",
    "                nzx, nzy, nz_val = sp.find(contingency)\n",
    "\n",
    "            contingency_sum = contingency.sum()\n",
    "            \n",
    "            # pi = Probabilitas kemunculan setiap nilai Fitur, pj = Probabilitas kemunculan kelas\n",
    "            pi = np.ravel(contingency.sum(axis=1))\n",
    "            pj = np.ravel(contingency.sum(axis=0))            \n",
    "            log_contingency_nm = np.log(nz_val)\n",
    "            \n",
    "            # Probabilitas nilai fitur terhadap kelas dibagi total data\n",
    "            contingency_nm = nz_val / contingency_sum\n",
    "            \n",
    "            outer = (pi.take(nzx).astype(np.int64, copy=False) * pj.take(nzy).astype(np.int64, copy=False))            \n",
    "            log_outer = -np.log(outer) + log(pi.sum()) + log(pj.sum())\n",
    "            \n",
    "            mi_score = (contingency_nm * (log_contingency_nm - log(contingency_sum)) +\n",
    "                contingency_nm * log_outer)\n",
    "            mi.append(mi_score.sum())\n",
    "\n",
    "        res = dict(zip(cv.get_feature_names(), np.array(mi)))\n",
    "        \n",
    "        columns = cv.get_feature_names()\n",
    "        \n",
    "        feature = {}\n",
    "        feature['vocabulary'] = []\n",
    "        feature['score'] = []\n",
    "        for i in range(len(res)):\n",
    "            feature['vocabulary'].append(columns[i])\n",
    "            feature['score'].append(res.get(columns[i]))\n",
    "            \n",
    "        feature_data['vocabulary'] = feature['vocabulary']\n",
    "        feature_data['score_feature'] = feature['score']\n",
    "        \n",
    "        feature_data = self.sort_score_feature(feature_data)\n",
    "        return feature_data\n",
    "    \n",
    "    def output_data_feature_selection(self, feature_data, filename, k):\n",
    "        filepath = os.getcwd()+'/Data/Data_Feature_Selection/'+filename+str(k)+'.csv'\n",
    "        # filepath = os.getcwd()+'/drive/My Drive/Colab Notebooks/New_Data/Data_Feature_Selection/'+filename+str(k)+'.csv'\n",
    "        raw_data = {'Feature': feature_data['vocabulary'],\n",
    "                   'Score': feature_data['score_feature']}\n",
    "        df = pandas.DataFrame(raw_data, columns = ['Feature', 'Score'])\n",
    "        df.to_csv(filepath, index=False)\n",
    "\n",
    "    def load_data_set_split(self, iteration):\n",
    "        data_review = []\n",
    "        data_class = []\n",
    "        filename = os.getcwd()+'/Data/Preprocessing/train/datatrain_seleksi_fitur'+str(iteration)+'.csv'\n",
    "        # filename = os.getcwd()+'/drive/My Drive/Colab Notebooks/New_Data/Preprocessing/train/datatrain_seleksi_fitur'+str(iteration)+'.csv'\n",
    "        names = ['Review', 'Class']\n",
    "        data = pandas.read_csv(filename, names=names, header=None, skiprows=1)\n",
    "        dataset_review = data.Review\n",
    "        for review in dataset_review:\n",
    "            review_split = review.split()\n",
    "            bag_word = []\n",
    "            for word in review_split:\n",
    "                bag_word.append(word)\n",
    "            data_review.append(bag_word)\n",
    "        data_class.extend(data.Class)\n",
    "            \n",
    "        return data_review, data_class\n",
    "\n",
    "    def __init__(self, feature_selection):\n",
    "        print(\"----------------------------------------------------\")\n",
    "        print(\"        F E A T U R E       S E L E C T I O N       \")\n",
    "        print(\"----------------------------------------------------\")    \n",
    "\n",
    "        for i in range(1,11):\n",
    "            data_review, data_class = self.load_data_set_split(i)\n",
    "\n",
    "            data_set_split = {}\n",
    "            data_set_split['review'] = data_review\n",
    "            data_set_split['class'] = data_class\n",
    "            \n",
    "            print(\"K-\",i,\" Jumlah Data : \", len(data_review))\n",
    "            \n",
    "            feature_data = {}\n",
    "            \n",
    "            if feature_selection == \"IG\":\n",
    "                feature_data['vocabulary'] = self.input_vocabulary(data_set_split)\n",
    "                feature_data.update(self.num_class(feature_data, data_set_split))\n",
    "                feature_data.update(self.information_gain(feature_data, data_set_split, i))\n",
    "                filename = 'Information_Gain/hasil_seleksi_fitur_'\n",
    "            else:\n",
    "                feature_data.update(self.mutual_information(feature_data, data_set_split))\n",
    "                filename = 'Mutual_Information/hasil_seleksi_fitur_'\n",
    "            \n",
    "            self.output_data_feature_selection(feature_data, filename, i)\n",
    "        print(\"--- FEATURE SELECTION DONE \\u2713 ---\")\n",
    "       \n",
    "'''\n",
    "----------------------------------------------------\n",
    "            Test Function Feature Selection\n",
    "----------------------------------------------------\n",
    "'''\n",
    "#feature_selection = IG / MI\n",
    "Feature_Selection(feature_selection=\"MI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n##  Test Function Classification  ##\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "================================================================================================\n",
    "                                C L A S S I F I C A T I O N\n",
    "================================================================================================\n",
    "\n",
    "'''\n",
    "\n",
    "class Classification():\n",
    "        \n",
    "    def load_dataset(self, filename):\n",
    "        data_review = []\n",
    "        data_class =  []     \n",
    "        filepath = os.getcwd()+'/Data/Preprocessing/'+filename+'.csv'\n",
    "        names = ['Review', 'Class']\n",
    "        data = pandas.read_csv(filepath, names=names, header=None, skiprows=1)\n",
    "        for review in data.Review:\n",
    "            data_review.append(review)\n",
    "        for label in data.Class:\n",
    "            data_class.append(label)\n",
    "\n",
    "        return data_review, data_class\n",
    "    \n",
    "    def select_best_feature(self, type, feature_vocab, feature_score, max_features, threshold):\n",
    "        new_features = []\n",
    "        for i in range(len(feature_vocab)):\n",
    "            vocabulary = feature_vocab[i]\n",
    "            if type=='Threshold':\n",
    "                if feature_score[i] >= threshold:\n",
    "                    new_features.append(vocabulary)\n",
    "            else :\n",
    "                if len(new_features) < max_features:\n",
    "                    new_features.append(vocabulary)\n",
    "                        \n",
    "        return new_features\n",
    "    \n",
    "    def make_file_output(self, selection_feature, n):\n",
    "        filename = \"Output_Classification_{0}_{1}\".format(selection_feature, n)\n",
    "        filepath = os.getcwd()+'/Data/Classification_Result/'+filename+'.txt'\n",
    "        text_file = open(filepath, \"w\")\n",
    "        text_file.write(\"\")\n",
    "        text_file.close()\n",
    "        \n",
    "    def output_to_text(self, data, selection_feature, n):\n",
    "        filename = \"Output_Classification_{0}_{1}\".format(selection_feature, n)\n",
    "        filepath = os.getcwd()+'/Data/Classification_Result/'+filename+'.txt'\n",
    "        text_file = open(filepath, \"a+\")\n",
    "        text_file.write(data)\n",
    "        text_file.close()\n",
    "        \n",
    "    def save_object(self, obj, filename):\n",
    "#         filepath = os.getcwd()+'/drive/Colab Notebooks/Data/Classification/'+filename+'.pkl'\n",
    "        filepath = os.getcwd()+'/Data/Classification_Result/Pickle/'+filename+'.pkl'\n",
    "        with open(filepath, 'wb') as output:  # Overwrites any existing file.\n",
    "            pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    def load_object(self, filename):\n",
    "#         filepath = os.getcwd()+'/drive/Colab Notebooks/Data/Classification/'+filename+'.pkl'\n",
    "        filepath = os.getcwd()+'/Data/Classification_Result/Pickle/'+filename+'.pkl'\n",
    "        with open(filepath, 'rb') as input:\n",
    "            obj = pickle.load(input)\n",
    "            return obj\n",
    "    \n",
    "    \n",
    "    def classification_training(self, preprocessing, feature_selection, threshold, max_features, c_value, gamma_value, degree_value, kernel):\n",
    "    \n",
    "        totalsvm = 0\n",
    "        totalDataset = 0\n",
    "        totalMatSvm = np.zeros((2,2))\n",
    "        \n",
    "        list_accuracy = []\n",
    "        \n",
    "        metrics = {}\n",
    "        metrics['accuracy_score'] = 0\n",
    "        metrics['precision_score'] = 0\n",
    "        metrics['recall_score'] = 0 \n",
    "        metrics['f1_score'] = 0   \n",
    "        \n",
    "        TFIDF_Model = []\n",
    "        SVM_Model = []\n",
    "        \n",
    "        self.make_file_output(feature_selection, round(max_features*100))\n",
    "        \n",
    "        text_to_save = \"\"\n",
    "        text_to_save += \"\\n=============================\\n\"\n",
    "        text_to_save += \"Preprocessing: {0}, Fitur Seleksi: {1}, Max Feature: {2}%\".format(preprocessing, feature_selection, round(max_features*100))\n",
    "        text_to_save += \"\\n--- K-FOLD CROSS VALIDATION ---\"\n",
    "        \n",
    "        print(text_to_save)\n",
    "        self.output_to_text(text_to_save, feature_selection, round(max_features*100))\n",
    "    \n",
    "        for index in range(1,11):\n",
    "            filename = 'train/datatrain_seleksi_fitur'+str(index)\n",
    "            X_train, train_y = self.load_dataset(filename)\n",
    "                    \n",
    "            filename = 'test/datatest_seleksi_fitur'+str(index)\n",
    "            X_test, test_y = self.load_dataset(filename)\n",
    "            \n",
    "            totalDataset = len(X_train)+len(X_test)\n",
    "            \n",
    "            text_to_save = \"\\n\"\n",
    "            text_to_save += \"----  K-Fold: {0} ----\".format(index)\n",
    "            print(text_to_save)\n",
    "            self.output_to_text(text_to_save, feature_selection, round(max_features*100))\n",
    "            \n",
    "            if feature_selection != 'No' :\n",
    "                 # Load Dataset IG Result\n",
    "                if feature_selection == \"IG\":\n",
    "                    filename = 'Information_Gain/hasil_seleksi_fitur_'\n",
    "                else:\n",
    "                    filename = 'Mutual_Information/hasil_seleksi_fitur_'\n",
    "                filepath = os.getcwd()+'/Data/Data_Feature_Selection/'+filename+str(index)+'.csv'\n",
    "                names = ['Feature', 'Score']\n",
    "                data = pandas.read_csv(filepath, names=names, header=None, skiprows=1, delimiter=\",\")\n",
    "                feature_vocab = data.Feature \n",
    "                feature_score = data.Score\n",
    "                \n",
    "                # Select Best Feature from All Feature \n",
    "                # type = 'Threshold' / 'Num Feature'\n",
    "                best_features = self.select_best_feature('Num Feature', feature_vocab, feature_score, round(max_features*len(feature_vocab)), threshold=threshold)\n",
    "\n",
    "                text_to_save = \"\\n\"\n",
    "                text_to_save += \"Features : {0}, Used : {1}\".format(len(feature_vocab), len(best_features))\n",
    "                print(text_to_save)\n",
    "                self.output_to_text(text_to_save, feature_selection, round(max_features*100))\n",
    "                \n",
    "                tfidf = TfidfVectorizer(sublinear_tf=True, use_idf=True, vocabulary=best_features)            \n",
    "                train_X = tfidf.fit_transform(X_train)\n",
    "                \n",
    "                TFIDF_Model.append(tfidf)\n",
    "            \n",
    "                test_X = tfidf.transform(X_test)\n",
    "                        \n",
    "            else:\n",
    "              \n",
    "                tfidf = TfidfVectorizer(sublinear_tf=True, use_idf=True)            \n",
    "                train_X = tfidf.fit_transform(X_train)\n",
    "                \n",
    "                TFIDF_Model.append(tfidf)\n",
    "                \n",
    "                test_X = tfidf.transform(X_test)\n",
    "            \n",
    "            # Train Classifier\n",
    "            if kernel == 'rbf':\n",
    "                svmClf = SVC(kernel='rbf', gamma=gamma_value, C=c_value)\n",
    "            elif kernel == 'linear':\n",
    "                svmClf = LinearSVC(random_state=0, dual=False)\n",
    "            elif kernel == 'poly':\n",
    "                svmClf = SVC(kernel='poly', degree=degree_value, C=c_value)\n",
    "                \n",
    "            svmClf.fit(train_X, train_y)\n",
    "            \n",
    "            SVM_Model.append(svmClf)\n",
    "        \n",
    "            resultSVM = svmClf.predict(test_X)            \n",
    "            totalsvm = totalsvm+sum(test_y==resultSVM)\n",
    "            \n",
    "            text_to_save = \"\\n\"\n",
    "            acc = accuracy_score(test_y, resultSVM)\n",
    "            text_to_save += \"Accuracy SVM : {0}\".format(acc)\n",
    "            list_accuracy.append(acc)\n",
    "            \n",
    "            print(text_to_save)\n",
    "            self.output_to_text(text_to_save, feature_selection, round(max_features*100))\n",
    "            \n",
    "            \"-- Confusion Matrix --\"\n",
    "            totalMatSvm += confusion_matrix(test_y, resultSVM)\n",
    "        \n",
    "        \"-- Metrics Score --\"\n",
    "        tn, fp, fn, tp = totalMatSvm.ravel()\n",
    "        metrics['accuracy_score'] = (tp + tn)/totalDataset\n",
    "        metrics['precision_score'] = tp/(tp + fp)\n",
    "        metrics['recall_score'] = tp / (tp + fn)\n",
    "        metrics['f1_score'] = 2*tp / (2*tp + fp + fn)\n",
    "        \n",
    "        text_to_save = \"\\n\"\n",
    "        text_to_save += \"=============================\\n\"\n",
    "        text_to_save += \"Parameter : C={0}, Gamma={1}, Degree={2}\\n\".format(c_value, gamma_value, degree_value)\n",
    "        text_to_save += \"Jumlah Data : {0}\\n\".format(totalDataset)\n",
    "        text_to_save += \"Akurasi SVM : {0}\\n\".format(totalsvm/totalDataset*100)\n",
    "        text_to_save += \"Metrics : {0}\".format(metrics)\n",
    "        \n",
    "        print(text_to_save)\n",
    "        self.output_to_text(text_to_save, feature_selection, round(max_features*100))\n",
    "        \n",
    "        best_acc = max(list_accuracy)\n",
    "        for idx in range(len(list_accuracy)):\n",
    "            if best_acc == list_accuracy[idx]:\n",
    "                filename = \"TF-IDF/TF-IDF_Train_{0}_N_{1}\".format(feature_selection, round(max_features*100))\n",
    "                self.save_object(TFIDF_Model[idx], filename)\n",
    "                filename = \"SVM/SVM_{0}_Classifier_N_{1}\".format(feature_selection, round(max_features*100))\n",
    "                self.save_object(SVM_Model[idx], filename)\n",
    "                break\n",
    "                \n",
    "    \n",
    "    def get_result(self):\n",
    "        return self.results['accuracy']\n",
    "                \n",
    "    def __init__ (self, preprocessing, feature_selection, threshold, max_features, c_variant, gamma_variant, degree_variant, kernel):\n",
    "        print(\"----------------------------------------------------\")\n",
    "        print(\"         S V M   C L A S S I F I C A T I O N        \")\n",
    "        print(\"----------------------------------------------------\")  \n",
    "        \n",
    "        self.results= {}\n",
    "        self.results['accuracy'] = []\n",
    "        self.results['kernel'] = []\n",
    "        self.results['c_value'] = []\n",
    "        self.results['gamma'] = []\n",
    "        self.results['degree'] = []\n",
    "        self.results['max_features'] = []\n",
    "        \n",
    "        best_kfold = 0\n",
    "        \n",
    "        if kernel == 'rbf':\n",
    "            for c_value in c_variant:\n",
    "                for gamma_value in gamma_variant:\n",
    "                    self.classification_training(preprocessing, feature_selection, threshold, max_features, c_value=c_value, gamma_value=gamma_value, degree_value=3, kernel=kernel)\n",
    "        elif kernel == 'linear':\n",
    "            gamma='scale'\n",
    "            for c_value in c_variant:\n",
    "                self.classification_training(preprocessing, feature_selection, threshold, max_features, c_value=c_value, gamma_value=gamma, degree_value=3, kernel=kernel)\n",
    "        elif kernel == 'poly':\n",
    "            for degree_value in degree_variant:\n",
    "                for c_value in c_variant:\n",
    "                    self.classification_training(preprocessing, feature_selection, threshold, max_features, c_value=c_value, gamma_value=0, degree_value=degree_value, kernel=kernel)\n",
    "        \n",
    "        \n",
    "\n",
    "'''\n",
    "##  Test Function Classification  ##\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test SVM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "         S V M   C L A S S I F I C A T I O N        \n",
      "----------------------------------------------------\n",
      "\n",
      "=============================\n",
      "Preprocessing: Stopword, Fitur Seleksi: MI, Max Feature: 10%\n",
      "--- K-FOLD CROSS VALIDATION ---\n",
      "\n",
      "----  K-Fold: 1 ----\n",
      "\n",
      "Features : 44700, Used : 4470\n",
      "\n",
      "Accuracy SVM : 0.865\n",
      "\n",
      "----  K-Fold: 2 ----\n",
      "\n",
      "Features : 44833, Used : 4483\n",
      "\n",
      "Accuracy SVM : 0.865\n",
      "\n",
      "----  K-Fold: 3 ----\n",
      "\n",
      "Features : 44754, Used : 4475\n",
      "\n",
      "Accuracy SVM : 0.88\n",
      "\n",
      "----  K-Fold: 4 ----\n",
      "\n",
      "Features : 45072, Used : 4507\n",
      "\n",
      "Accuracy SVM : 0.855\n",
      "\n",
      "----  K-Fold: 5 ----\n",
      "\n",
      "Features : 44868, Used : 4487\n",
      "\n",
      "Accuracy SVM : 0.855\n",
      "\n",
      "----  K-Fold: 6 ----\n",
      "\n",
      "Features : 44659, Used : 4466\n",
      "\n",
      "Accuracy SVM : 0.9\n",
      "\n",
      "----  K-Fold: 7 ----\n",
      "\n",
      "Features : 44466, Used : 4447\n",
      "\n",
      "Accuracy SVM : 0.88\n",
      "\n",
      "----  K-Fold: 8 ----\n",
      "\n",
      "Features : 44644, Used : 4464\n",
      "\n",
      "Accuracy SVM : 0.865\n",
      "\n",
      "----  K-Fold: 9 ----\n",
      "\n",
      "Features : 44781, Used : 4478\n",
      "\n",
      "Accuracy SVM : 0.845\n",
      "\n",
      "----  K-Fold: 10 ----\n",
      "\n",
      "Features : 44625, Used : 4462\n",
      "\n",
      "Accuracy SVM : 0.905\n",
      "\n",
      "=============================\n",
      "Parameter : C=1, Gamma=scale, Degree=3\n",
      "Jumlah Data : 2000\n",
      "Akurasi SVM : 87.15\n",
      "Metrics : {'accuracy_score': 0.8715, 'precision_score': 0.872617853560682, 'recall_score': 0.87, 'f1_score': 0.871306960440661}\n",
      "----------------------------------------------------\n",
      "         S V M   C L A S S I F I C A T I O N        \n",
      "----------------------------------------------------\n",
      "\n",
      "=============================\n",
      "Preprocessing: Stopword, Fitur Seleksi: MI, Max Feature: 20%\n",
      "--- K-FOLD CROSS VALIDATION ---\n",
      "\n",
      "----  K-Fold: 1 ----\n",
      "\n",
      "Features : 44700, Used : 8940\n",
      "\n",
      "Accuracy SVM : 0.875\n",
      "\n",
      "----  K-Fold: 2 ----\n",
      "\n",
      "Features : 44833, Used : 8967\n",
      "\n",
      "Accuracy SVM : 0.9\n",
      "\n",
      "----  K-Fold: 3 ----\n",
      "\n",
      "Features : 44754, Used : 8951\n",
      "\n",
      "Accuracy SVM : 0.89\n",
      "\n",
      "----  K-Fold: 4 ----\n",
      "\n",
      "Features : 45072, Used : 9014\n",
      "\n",
      "Accuracy SVM : 0.87\n",
      "\n",
      "----  K-Fold: 5 ----\n",
      "\n",
      "Features : 44868, Used : 8974\n",
      "\n",
      "Accuracy SVM : 0.875\n",
      "\n",
      "----  K-Fold: 6 ----\n",
      "\n",
      "Features : 44659, Used : 8932\n",
      "\n",
      "Accuracy SVM : 0.895\n",
      "\n",
      "----  K-Fold: 7 ----\n",
      "\n",
      "Features : 44466, Used : 8893\n",
      "\n",
      "Accuracy SVM : 0.875\n",
      "\n",
      "----  K-Fold: 8 ----\n",
      "\n",
      "Features : 44644, Used : 8929\n",
      "\n",
      "Accuracy SVM : 0.87\n",
      "\n",
      "----  K-Fold: 9 ----\n",
      "\n",
      "Features : 44781, Used : 8956\n",
      "\n",
      "Accuracy SVM : 0.845\n",
      "\n",
      "----  K-Fold: 10 ----\n",
      "\n",
      "Features : 44625, Used : 8925\n",
      "\n",
      "Accuracy SVM : 0.915\n",
      "\n",
      "=============================\n",
      "Parameter : C=1, Gamma=scale, Degree=3\n",
      "Jumlah Data : 2000\n",
      "Akurasi SVM : 88.1\n",
      "Metrics : {'accuracy_score': 0.881, 'precision_score': 0.8764822134387352, 'recall_score': 0.887, 'f1_score': 0.8817097415506958}\n",
      "----------------------------------------------------\n",
      "         S V M   C L A S S I F I C A T I O N        \n",
      "----------------------------------------------------\n",
      "\n",
      "=============================\n",
      "Preprocessing: Stopword, Fitur Seleksi: MI, Max Feature: 30%\n",
      "--- K-FOLD CROSS VALIDATION ---\n",
      "\n",
      "----  K-Fold: 1 ----\n",
      "\n",
      "Features : 44700, Used : 13410\n",
      "\n",
      "Accuracy SVM : 0.86\n",
      "\n",
      "----  K-Fold: 2 ----\n",
      "\n",
      "Features : 44833, Used : 13450\n",
      "\n",
      "Accuracy SVM : 0.905\n",
      "\n",
      "----  K-Fold: 3 ----\n",
      "\n",
      "Features : 44754, Used : 13426\n",
      "\n",
      "Accuracy SVM : 0.895\n",
      "\n",
      "----  K-Fold: 4 ----\n",
      "\n",
      "Features : 45072, Used : 13522\n",
      "\n",
      "Accuracy SVM : 0.88\n",
      "\n",
      "----  K-Fold: 5 ----\n",
      "\n",
      "Features : 44868, Used : 13460\n",
      "\n",
      "Accuracy SVM : 0.865\n",
      "\n",
      "----  K-Fold: 6 ----\n",
      "\n",
      "Features : 44659, Used : 13398\n",
      "\n",
      "Accuracy SVM : 0.895\n",
      "\n",
      "----  K-Fold: 7 ----\n",
      "\n",
      "Features : 44466, Used : 13340\n",
      "\n",
      "Accuracy SVM : 0.88\n",
      "\n",
      "----  K-Fold: 8 ----\n",
      "\n",
      "Features : 44644, Used : 13393\n",
      "\n",
      "Accuracy SVM : 0.875\n",
      "\n",
      "----  K-Fold: 9 ----\n",
      "\n",
      "Features : 44781, Used : 13434\n",
      "\n",
      "Accuracy SVM : 0.85\n",
      "\n",
      "----  K-Fold: 10 ----\n",
      "\n",
      "Features : 44625, Used : 13388\n",
      "\n",
      "Accuracy SVM : 0.905\n",
      "\n",
      "=============================\n",
      "Parameter : C=1, Gamma=scale, Degree=3\n",
      "Jumlah Data : 2000\n",
      "Akurasi SVM : 88.1\n",
      "Metrics : {'accuracy_score': 0.881, 'precision_score': 0.8742632612966601, 'recall_score': 0.89, 'f1_score': 0.8820614469772051}\n",
      "----------------------------------------------------\n",
      "         S V M   C L A S S I F I C A T I O N        \n",
      "----------------------------------------------------\n",
      "\n",
      "=============================\n",
      "Preprocessing: Stopword, Fitur Seleksi: MI, Max Feature: 40%\n",
      "--- K-FOLD CROSS VALIDATION ---\n",
      "\n",
      "----  K-Fold: 1 ----\n",
      "\n",
      "Features : 44700, Used : 17880\n",
      "\n",
      "Accuracy SVM : 0.85\n",
      "\n",
      "----  K-Fold: 2 ----\n",
      "\n",
      "Features : 44833, Used : 17933\n",
      "\n",
      "Accuracy SVM : 0.905\n",
      "\n",
      "----  K-Fold: 3 ----\n",
      "\n",
      "Features : 44754, Used : 17902\n",
      "\n",
      "Accuracy SVM : 0.89\n",
      "\n",
      "----  K-Fold: 4 ----\n",
      "\n",
      "Features : 45072, Used : 18029\n",
      "\n",
      "Accuracy SVM : 0.88\n",
      "\n",
      "----  K-Fold: 5 ----\n",
      "\n",
      "Features : 44868, Used : 17947\n",
      "\n",
      "Accuracy SVM : 0.88\n",
      "\n",
      "----  K-Fold: 6 ----\n",
      "\n",
      "Features : 44659, Used : 17864\n",
      "\n",
      "Accuracy SVM : 0.895\n",
      "\n",
      "----  K-Fold: 7 ----\n",
      "\n",
      "Features : 44466, Used : 17786\n",
      "\n",
      "Accuracy SVM : 0.885\n",
      "\n",
      "----  K-Fold: 8 ----\n",
      "\n",
      "Features : 44644, Used : 17858\n",
      "\n",
      "Accuracy SVM : 0.895\n",
      "\n",
      "----  K-Fold: 9 ----\n",
      "\n",
      "Features : 44781, Used : 17912\n",
      "\n",
      "Accuracy SVM : 0.855\n",
      "\n",
      "----  K-Fold: 10 ----\n",
      "\n",
      "Features : 44625, Used : 17850\n",
      "\n",
      "Accuracy SVM : 0.93\n",
      "\n",
      "=============================\n",
      "Parameter : C=1, Gamma=scale, Degree=3\n",
      "Jumlah Data : 2000\n",
      "Akurasi SVM : 88.64999999999999\n",
      "Metrics : {'accuracy_score': 0.8865, 'precision_score': 0.8830525272547076, 'recall_score': 0.891, 'f1_score': 0.8870084619213539}\n",
      "----------------------------------------------------\n",
      "         S V M   C L A S S I F I C A T I O N        \n",
      "----------------------------------------------------\n",
      "\n",
      "=============================\n",
      "Preprocessing: Stopword, Fitur Seleksi: MI, Max Feature: 50%\n",
      "--- K-FOLD CROSS VALIDATION ---\n",
      "\n",
      "----  K-Fold: 1 ----\n",
      "\n",
      "Features : 44700, Used : 22350\n",
      "\n",
      "Accuracy SVM : 0.86\n",
      "\n",
      "----  K-Fold: 2 ----\n",
      "\n",
      "Features : 44833, Used : 22416\n",
      "\n",
      "Accuracy SVM : 0.91\n",
      "\n",
      "----  K-Fold: 3 ----\n",
      "\n",
      "Features : 44754, Used : 22377\n",
      "\n",
      "Accuracy SVM : 0.89\n",
      "\n",
      "----  K-Fold: 4 ----\n",
      "\n",
      "Features : 45072, Used : 22536\n",
      "\n",
      "Accuracy SVM : 0.88\n",
      "\n",
      "----  K-Fold: 5 ----\n",
      "\n",
      "Features : 44868, Used : 22434\n",
      "\n",
      "Accuracy SVM : 0.875\n",
      "\n",
      "----  K-Fold: 6 ----\n",
      "\n",
      "Features : 44659, Used : 22330\n",
      "\n",
      "Accuracy SVM : 0.895\n",
      "\n",
      "----  K-Fold: 7 ----\n",
      "\n",
      "Features : 44466, Used : 22233\n",
      "\n",
      "Accuracy SVM : 0.885\n",
      "\n",
      "----  K-Fold: 8 ----\n",
      "\n",
      "Features : 44644, Used : 22322\n",
      "\n",
      "Accuracy SVM : 0.895\n",
      "\n",
      "----  K-Fold: 9 ----\n",
      "\n",
      "Features : 44781, Used : 22390\n",
      "\n",
      "Accuracy SVM : 0.86\n",
      "\n",
      "----  K-Fold: 10 ----\n",
      "\n",
      "Features : 44625, Used : 22312\n",
      "\n",
      "Accuracy SVM : 0.925\n",
      "\n",
      "=============================\n",
      "Parameter : C=1, Gamma=scale, Degree=3\n",
      "Jumlah Data : 2000\n",
      "Akurasi SVM : 88.75\n",
      "Metrics : {'accuracy_score': 0.8875, 'precision_score': 0.8832838773491593, 'recall_score': 0.893, 'f1_score': 0.8881153654898061}\n",
      "----------------------------------------------------\n",
      "         S V M   C L A S S I F I C A T I O N        \n",
      "----------------------------------------------------\n",
      "\n",
      "=============================\n",
      "Preprocessing: Stopword, Fitur Seleksi: MI, Max Feature: 60%\n",
      "--- K-FOLD CROSS VALIDATION ---\n",
      "\n",
      "----  K-Fold: 1 ----\n",
      "\n",
      "Features : 44700, Used : 26820\n",
      "\n",
      "Accuracy SVM : 0.855\n",
      "\n",
      "----  K-Fold: 2 ----\n",
      "\n",
      "Features : 44833, Used : 26900\n",
      "\n",
      "Accuracy SVM : 0.9\n",
      "\n",
      "----  K-Fold: 3 ----\n",
      "\n",
      "Features : 44754, Used : 26852\n",
      "\n",
      "Accuracy SVM : 0.89\n",
      "\n",
      "----  K-Fold: 4 ----\n",
      "\n",
      "Features : 45072, Used : 27043\n",
      "\n",
      "Accuracy SVM : 0.88\n",
      "\n",
      "----  K-Fold: 5 ----\n",
      "\n",
      "Features : 44868, Used : 26921\n",
      "\n",
      "Accuracy SVM : 0.87\n",
      "\n",
      "----  K-Fold: 6 ----\n",
      "\n",
      "Features : 44659, Used : 26795\n",
      "\n",
      "Accuracy SVM : 0.895\n",
      "\n",
      "----  K-Fold: 7 ----\n",
      "\n",
      "Features : 44466, Used : 26680\n",
      "\n",
      "Accuracy SVM : 0.895\n",
      "\n",
      "----  K-Fold: 8 ----\n",
      "\n",
      "Features : 44644, Used : 26786\n",
      "\n",
      "Accuracy SVM : 0.895\n",
      "\n",
      "----  K-Fold: 9 ----\n",
      "\n",
      "Features : 44781, Used : 26869\n",
      "\n",
      "Accuracy SVM : 0.86\n",
      "\n",
      "----  K-Fold: 10 ----\n",
      "\n",
      "Features : 44625, Used : 26775\n",
      "\n",
      "Accuracy SVM : 0.92\n",
      "\n",
      "=============================\n",
      "Parameter : C=1, Gamma=scale, Degree=3\n",
      "Jumlah Data : 2000\n",
      "Akurasi SVM : 88.6\n",
      "Metrics : {'accuracy_score': 0.886, 'precision_score': 0.8821782178217822, 'recall_score': 0.891, 'f1_score': 0.8865671641791045}\n",
      "----------------------------------------------------\n",
      "         S V M   C L A S S I F I C A T I O N        \n",
      "----------------------------------------------------\n",
      "\n",
      "=============================\n",
      "Preprocessing: Stopword, Fitur Seleksi: MI, Max Feature: 70%\n",
      "--- K-FOLD CROSS VALIDATION ---\n",
      "\n",
      "----  K-Fold: 1 ----\n",
      "\n",
      "Features : 44700, Used : 31290\n",
      "\n",
      "Accuracy SVM : 0.865\n",
      "\n",
      "----  K-Fold: 2 ----\n",
      "\n",
      "Features : 44833, Used : 31383\n",
      "\n",
      "Accuracy SVM : 0.9\n",
      "\n",
      "----  K-Fold: 3 ----\n",
      "\n",
      "Features : 44754, Used : 31328\n",
      "\n",
      "Accuracy SVM : 0.89\n",
      "\n",
      "----  K-Fold: 4 ----\n",
      "\n",
      "Features : 45072, Used : 31550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy SVM : 0.88\n",
      "\n",
      "----  K-Fold: 5 ----\n",
      "\n",
      "Features : 44868, Used : 31408\n",
      "\n",
      "Accuracy SVM : 0.87\n",
      "\n",
      "----  K-Fold: 6 ----\n",
      "\n",
      "Features : 44659, Used : 31261\n",
      "\n",
      "Accuracy SVM : 0.9\n",
      "\n",
      "----  K-Fold: 7 ----\n",
      "\n",
      "Features : 44466, Used : 31126\n",
      "\n",
      "Accuracy SVM : 0.895\n",
      "\n",
      "----  K-Fold: 8 ----\n",
      "\n",
      "Features : 44644, Used : 31251\n",
      "\n",
      "Accuracy SVM : 0.905\n",
      "\n",
      "----  K-Fold: 9 ----\n",
      "\n",
      "Features : 44781, Used : 31347\n",
      "\n",
      "Accuracy SVM : 0.86\n",
      "\n",
      "----  K-Fold: 10 ----\n",
      "\n",
      "Features : 44625, Used : 31237\n",
      "\n",
      "Accuracy SVM : 0.925\n",
      "\n",
      "=============================\n",
      "Parameter : C=1, Gamma=scale, Degree=3\n",
      "Jumlah Data : 2000\n",
      "Akurasi SVM : 88.9\n",
      "Metrics : {'accuracy_score': 0.889, 'precision_score': 0.8851485148514852, 'recall_score': 0.894, 'f1_score': 0.8895522388059701}\n",
      "----------------------------------------------------\n",
      "         S V M   C L A S S I F I C A T I O N        \n",
      "----------------------------------------------------\n",
      "\n",
      "=============================\n",
      "Preprocessing: Stopword, Fitur Seleksi: MI, Max Feature: 80%\n",
      "--- K-FOLD CROSS VALIDATION ---\n",
      "\n",
      "----  K-Fold: 1 ----\n",
      "\n",
      "Features : 44700, Used : 35760\n",
      "\n",
      "Accuracy SVM : 0.87\n",
      "\n",
      "----  K-Fold: 2 ----\n",
      "\n",
      "Features : 44833, Used : 35866\n",
      "\n",
      "Accuracy SVM : 0.895\n",
      "\n",
      "----  K-Fold: 3 ----\n",
      "\n",
      "Features : 44754, Used : 35803\n",
      "\n",
      "Accuracy SVM : 0.89\n",
      "\n",
      "----  K-Fold: 4 ----\n",
      "\n",
      "Features : 45072, Used : 36058\n",
      "\n",
      "Accuracy SVM : 0.88\n",
      "\n",
      "----  K-Fold: 5 ----\n",
      "\n",
      "Features : 44868, Used : 35894\n",
      "\n",
      "Accuracy SVM : 0.87\n",
      "\n",
      "----  K-Fold: 6 ----\n",
      "\n",
      "Features : 44659, Used : 35727\n",
      "\n",
      "Accuracy SVM : 0.9\n",
      "\n",
      "----  K-Fold: 7 ----\n",
      "\n",
      "Features : 44466, Used : 35573\n",
      "\n",
      "Accuracy SVM : 0.895\n",
      "\n",
      "----  K-Fold: 8 ----\n",
      "\n",
      "Features : 44644, Used : 35715\n",
      "\n",
      "Accuracy SVM : 0.9\n",
      "\n",
      "----  K-Fold: 9 ----\n",
      "\n",
      "Features : 44781, Used : 35825\n",
      "\n",
      "Accuracy SVM : 0.86\n",
      "\n",
      "----  K-Fold: 10 ----\n",
      "\n",
      "Features : 44625, Used : 35700\n",
      "\n",
      "Accuracy SVM : 0.93\n",
      "\n",
      "=============================\n",
      "Parameter : C=1, Gamma=scale, Degree=3\n",
      "Jumlah Data : 2000\n",
      "Akurasi SVM : 88.9\n",
      "Metrics : {'accuracy_score': 0.889, 'precision_score': 0.8843873517786561, 'recall_score': 0.895, 'f1_score': 0.889662027833002}\n",
      "----------------------------------------------------\n",
      "         S V M   C L A S S I F I C A T I O N        \n",
      "----------------------------------------------------\n",
      "\n",
      "=============================\n",
      "Preprocessing: Stopword, Fitur Seleksi: MI, Max Feature: 90%\n",
      "--- K-FOLD CROSS VALIDATION ---\n",
      "\n",
      "----  K-Fold: 1 ----\n",
      "\n",
      "Features : 44700, Used : 40230\n",
      "\n",
      "Accuracy SVM : 0.86\n",
      "\n",
      "----  K-Fold: 2 ----\n",
      "\n",
      "Features : 44833, Used : 40350\n",
      "\n",
      "Accuracy SVM : 0.88\n",
      "\n",
      "----  K-Fold: 3 ----\n",
      "\n",
      "Features : 44754, Used : 40279\n",
      "\n",
      "Accuracy SVM : 0.88\n",
      "\n",
      "----  K-Fold: 4 ----\n",
      "\n",
      "Features : 45072, Used : 40565\n",
      "\n",
      "Accuracy SVM : 0.88\n",
      "\n",
      "----  K-Fold: 5 ----\n",
      "\n",
      "Features : 44868, Used : 40381\n",
      "\n",
      "Accuracy SVM : 0.875\n",
      "\n",
      "----  K-Fold: 6 ----\n",
      "\n",
      "Features : 44659, Used : 40193\n",
      "\n",
      "Accuracy SVM : 0.895\n",
      "\n",
      "----  K-Fold: 7 ----\n",
      "\n",
      "Features : 44466, Used : 40019\n",
      "\n",
      "Accuracy SVM : 0.885\n",
      "\n",
      "----  K-Fold: 8 ----\n",
      "\n",
      "Features : 44644, Used : 40180\n",
      "\n",
      "Accuracy SVM : 0.9\n",
      "\n",
      "----  K-Fold: 9 ----\n",
      "\n",
      "Features : 44781, Used : 40303\n",
      "\n",
      "Accuracy SVM : 0.85\n",
      "\n",
      "----  K-Fold: 10 ----\n",
      "\n",
      "Features : 44625, Used : 40162\n",
      "\n",
      "Accuracy SVM : 0.92\n",
      "\n",
      "=============================\n",
      "Parameter : C=1, Gamma=scale, Degree=3\n",
      "Jumlah Data : 2000\n",
      "Akurasi SVM : 88.25\n",
      "Metrics : {'accuracy_score': 0.8825, 'precision_score': 0.8790882061446977, 'recall_score': 0.887, 'f1_score': 0.883026381284221}\n",
      "----------------------------------------------------\n",
      "         S V M   C L A S S I F I C A T I O N        \n",
      "----------------------------------------------------\n",
      "\n",
      "=============================\n",
      "Preprocessing: Stopword, Fitur Seleksi: MI, Max Feature: 100%\n",
      "--- K-FOLD CROSS VALIDATION ---\n",
      "\n",
      "----  K-Fold: 1 ----\n",
      "\n",
      "Features : 44700, Used : 44700\n",
      "\n",
      "Accuracy SVM : 0.87\n",
      "\n",
      "----  K-Fold: 2 ----\n",
      "\n",
      "Features : 44833, Used : 44833\n",
      "\n",
      "Accuracy SVM : 0.89\n",
      "\n",
      "----  K-Fold: 3 ----\n",
      "\n",
      "Features : 44754, Used : 44754\n",
      "\n",
      "Accuracy SVM : 0.88\n",
      "\n",
      "----  K-Fold: 4 ----\n",
      "\n",
      "Features : 45072, Used : 45072\n",
      "\n",
      "Accuracy SVM : 0.875\n",
      "\n",
      "----  K-Fold: 5 ----\n",
      "\n",
      "Features : 44868, Used : 44868\n",
      "\n",
      "Accuracy SVM : 0.87\n",
      "\n",
      "----  K-Fold: 6 ----\n",
      "\n",
      "Features : 44659, Used : 44659\n",
      "\n",
      "Accuracy SVM : 0.895\n",
      "\n",
      "----  K-Fold: 7 ----\n",
      "\n",
      "Features : 44466, Used : 44466\n",
      "\n",
      "Accuracy SVM : 0.885\n",
      "\n",
      "----  K-Fold: 8 ----\n",
      "\n",
      "Features : 44644, Used : 44644\n",
      "\n",
      "Accuracy SVM : 0.905\n",
      "\n",
      "----  K-Fold: 9 ----\n",
      "\n",
      "Features : 44781, Used : 44781\n",
      "\n",
      "Accuracy SVM : 0.855\n",
      "\n",
      "----  K-Fold: 10 ----\n",
      "\n",
      "Features : 44625, Used : 44625\n",
      "\n",
      "Accuracy SVM : 0.92\n",
      "\n",
      "=============================\n",
      "Parameter : C=1, Gamma=scale, Degree=3\n",
      "Jumlah Data : 2000\n",
      "Akurasi SVM : 88.44999999999999\n",
      "Metrics : {'accuracy_score': 0.8845, 'precision_score': 0.8803165182987142, 'recall_score': 0.89, 'f1_score': 0.8851317752362009}\n"
     ]
    }
   ],
   "source": [
    "list_max_features = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "for max_features in list_max_features:\n",
    "    result = Classification(\n",
    "            preprocessing = 'Stopword',\n",
    "            feature_selection = 'MI',\n",
    "            threshold = 1,\n",
    "            max_features = max_features,\n",
    "            degree_variant = [1],\n",
    "            c_variant = [1],\n",
    "            gamma_variant = [1],\n",
    "            kernel = 'linear'\n",
    "        ).get_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification by User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your data: a secret society so powerful it can get away with murder . a secret society so exclusive it firebrands everyone who joins with its mark . a secret society so secret . . . it has a big logo up on top of the building ! ? you know something is rotten with the skulls right from the get-go . i mean , what self-respecting prep school-ivy league snob would join an organization with a name as stupid as \" the skulls \" ? well , luke ( joshua jackson ) would be , for one . only he's no preppie . he's a \" townie \" with no money , but even though he's of the lower classes , since he's such a good rower ( yes , \" the skulls , \" i get it ) , he's a shoo-in for the secret society . a mysterious invitation arrives , and luke is whisked into a world of power and money , where men in red robes usher in beautiful women for the taking at tuxedoed parties . before you can utter \" fidelio , \" luke has become one of them . luke is soon partnered with a \" soul mate \" ( not making that up ) , caleb mandrake ( meet the deedles' paul walker ) with whom he is supposed to keep no secrets . but uh-oh ! when luke's roommate gets jealous and threatens to expose the entire society when he steals caleb's key to the secret chambers ( remember , that big skull on the roof points the way in ) . a cover-up ensues and luke romances \" popular \" star leslie bibb , and then come a grand series of plot twists so asinine you'll want to join in with the laughter and mockery of the audience if only doing so didn't make this movie even worse . its desperate earnestness makes it even more laughable ( with none other than craig t . \" coach \" nelson lording over it all ) , and by the time 45 minutes are up , you'll probably be ready to leave . i know i was . unfortunately , teens don't read movie reviews , so this critique will likely be lost on anyone who cares . would that they would put their collective foot down . the last thing we need is another bad movie that simply serves as an excuse to sell a soundtrack . \n",
      "----------------------------------------------------\n",
      "             P R E - P R O C E S S I N G            \n",
      "----------------------------------------------------\n",
      "--- PREPROCESSING DONE ✓ ---\n",
      "----------------------------------------------------\n",
      "         S V M   C L A S S I F I C A T I O N        \n",
      "----------------------------------------------------\n",
      "Hasil Sentimen : [-] Negatif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[========================================] 1/1 (100%) 0 to go\r",
      "[========================================] 1/1 (100%) 0 to go\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Classification_Input at 0x25bc4fa0dd8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Classification_Input():\n",
    "    \n",
    "    def load_object(self, filename):\n",
    "        filepath = os.getcwd()+'/Data/Classification_Result/Pickle/'+filename+'.pkl'\n",
    "        with open(filepath, 'rb') as input:\n",
    "            obj = pickle.load(input)\n",
    "            return obj\n",
    "\n",
    "    def do_classification(self, X_test, feature_selection, best_num_feature):\n",
    "        print(\"----------------------------------------------------\")\n",
    "        print(\"         S V M   C L A S S I F I C A T I O N        \")\n",
    "        print(\"----------------------------------------------------\")  \n",
    "        \n",
    "        # Load TF-IDF Model\n",
    "        filename = \"TF-IDF/TF-IDF_Train_{0}_N_{1}\".format(feature_selection, best_num_feature)\n",
    "        tfidf = self.load_object(filename)\n",
    "        test_X = tfidf.transform(X_test)\n",
    "        \n",
    "        # Load SVM Classifier Model\n",
    "        filename = \"SVM/SVM_{0}_Classifier_N_{1}\".format(feature_selection, best_num_feature)\n",
    "        svmClf = self.load_object(filename)\n",
    "        \n",
    "        resultSVM = svmClf.predict(test_X)\n",
    "        \n",
    "        if resultSVM == 1:\n",
    "            print(\"Hasil Sentimen : [+] Positif\")\n",
    "        else:\n",
    "            print(\"Hasil Sentimen : [-] Negatif\")\n",
    "\n",
    "    def input_data(self, feature_selection, best_num_feature):\n",
    "        data_input = input(\"Enter your data: \")\n",
    "        new_data = [data_input]\n",
    "        data_cleaned = Preprocessing(stopword = True, stem = False, x=new_data, y=[]).get_result()\n",
    "        self.do_classification(data_cleaned, feature_selection, best_num_feature)\n",
    "        \n",
    "    def __init__ (self, feature_selection, best_num_feature):\n",
    "        # SVM Classification By Input\n",
    "        \n",
    "        self.input_data(feature_selection, best_num_feature)\n",
    "\n",
    "Classification_Input(\"MI\", 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
